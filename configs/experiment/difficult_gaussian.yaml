# @package _global_

defaults:
  - override /mode: default.yaml
  - override /trainer: default.yaml
  - override /model: base.yaml
  - override /datamodule: difficult_gaussian
  - override /callbacks: null
  - override /logger: many_loggers.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "difficult_gaussian"

seed: 12345

num_steps: 100

sigma: 0.1
t_f: 1.0
lr: 1e-4

trainer:
  min_epochs: 1
  max_epochs: 100
  max_steps: 1500
  log_every_n_steps: 100
  # flush_logs_every_n_steps: 10

callbacks:
  sample:
    _target_: src.callbacks.metric_cb.VizSampleDist
    every_n: 150
  lr:
    _target_: src.callbacks.lr.LinearScheduler
  lr_m:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: epoch
